{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG arch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9Qxy1vsPvacV+8wspjNtj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"qvk1tT88DfXK","executionInfo":{"status":"error","timestamp":1628332549216,"user_tz":-180,"elapsed":298,"user":{"displayName":"itamar ginsberg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgNNu4rKWDIJbuQ6EjlVLGe3VMZEHSAKyW4bG5qQ=s64","userId":"14768070087940992282"}},"outputId":"ff497e23-1f81-4303-8d3e-37b1ae19fe4c"},"source":["\n","\n","\n","import torch\n","import torch.nn as nn\n","from .._internally_replaced_utils import load_state_dict_from_url\n","from typing import Union, List, Dict, Any, cast\n","\n","\n","__all__ = [\n","    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n","    'vgg19_bn', 'vgg19',\n","]\n","\n","\n","model_urls = {\n","    'vgg11': 'https://download.pytorch.org/models/vgg11-8a719046.pth',\n","    'vgg13': 'https://download.pytorch.org/models/vgg13-19584684.pth',\n","    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n","    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n","    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n","    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n","    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n","    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n","}\n","\n","\n","class VGG(nn.Module):\n","\n","    def __init__(\n","        self,\n","        features: nn.Module,\n","        num_classes: int = 1000,\n","        init_weights: bool = True\n","    ) -> None:\n","        super(VGG, self).__init__()\n","        self.features = features\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self) -> None:\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n","    layers: List[nn.Module] = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            v = cast(int, v)\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","\n","cfgs: Dict[str, List[Union[str, int]]] = {\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, **kwargs: Any) -> VGG:\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[arch],\n","                                              progress=progress)\n","        model.load_state_dict(state_dict)\n","    return model\n","\n","\n","def vgg11(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 11-layer model (configuration \"A\") from\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n","\n","\n","def vgg11_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n","\n","\n","def vgg13(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 13-layer model (configuration \"B\")\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n","\n","\n","def vgg13_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n","\n","\n","def vgg16(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 16-layer model (configuration \"D\")\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n","\n","\n","def vgg16_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n","\n","\n","def vgg19(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 19-layer model (configuration \"E\")\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n","\n","\n","def vgg19_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n","    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n","    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n","    The required minimum input size of the model is 32x32.\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2170b96a8d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internally_replaced_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}