{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer Learning - thumb / cat-dog classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMnBnC3hRB2C0SwSBYHKmaD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"OUvQfMTNG8Qb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPldDhFrDZOV"},"source":["## CATS AND DOGS EXAMPLE:\n","## https://medium.com/predict/using-pytorch-for-kaggles-famous-dogs-vs-cats-challenge-part-1-preprocessing-and-training-407017e1a10c\n","\n","# PART 1: ORGANIZING DATA\n","import os\n","train_dir = \"./data/train\"\n","train_dogs_dir = f'{train_dir}/dogs'\n","train_cats_dir = f'{train_dir}/cats'\n","val_dir = \"./data/val\"\n","val_dogs_dir = f'{val_dir}/dogs'\n","val_cats_dir = f'{val_dir}/cats'\n","print(\"Printing data dir\")\n","print(os.listdir(\"data\")) # Shows train, val folders are under data\n","print(\"Printing train dir\")\n","!ls {train_dir} | head -n 5 # Shows image files are in train folder\n","print(\"Printing train dog dir\")\n","!ls {train_dogs_dir} | head -n 5 # Check the (empty) folder exist\n","print(\"Printing train cat dir\")\n","!ls {train_cats_dir} | head -n 5 # Check the (empty) folder exist\n","print(\"Printing val dir\")\n","!ls {val_dir} | head -n 5  # Shows subfolder dogs and cats exist\n","print(\"Printing val dog dir\")\n","!ls {val_dogs_dir} | head -n 5 # Check the (empty) folder exist\n","print(\"Printing val cat dir\")\n","!ls {val_cats_dir} | head -n 5 # Check the (empty) folder exist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kc2F3dtUxC_"},"source":["import shutil\n","import re\n","files = os.listdir(train_dir)\n","# Move all train cat images to cats folder, dog images to dogs folder\n","for f in files:\n","    catSearchObj = re.search(\"cat\", f)\n","    dogSearchObj = re.search(\"dog\", f)\n","    if catSearchObj:\n","        shutil.move(f'{train_dir}/{f}', train_cats_dir)\n","    elif dogSearchObj:\n","        shutil.move(f'{train_dir}/{f}', train_dogs_dir)\n","\n","\n","print(\"Printing train dir\") # shows cats, dogs subfolders only\n","!ls {train_dir} | head -n 5\n","print(\"Printing train dog dir\") # there is now dog images in dogs folder\n","!ls {train_dogs_dir} | head -n 5\n","print(\"Printing train cat dir\") # there is now cat images in cats folder\n","!ls {train_cats_dir} | head -n 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"piK6GO7KVgnR"},"source":["files = os.listdir(train_dogs_dir)\n","for f in files:\n","    validationDogsSearchObj = re.search(\"4\", f)\n","    if validationDogsSearchObj:\n","        shutil.move(f'{train_dogs_dir}/{f}', val_dogs_dir)\n","print(\"Printing val dog dir\")\n","!ls {val_dogs_dir} | head -n 5\n","\n","files = os.listdir(train_cats_dir)\n","for f in files:\n","    validationCatsSearchObj = re.search(\"4\", f)\n","    if validationCatsSearchObj:\n","        shutil.move(f'{train_cats_dir}/{f}', val_cats_dir)\n","print(\"Printing val cat dir\")\n","!ls {val_cats_dir} | head -n 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hpc_Lqo5Wdmk","executionInfo":{"status":"ok","timestamp":1629644107351,"user_tz":-180,"elapsed":4679,"user":{"displayName":"itamar ginsberg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgNNu4rKWDIJbuQ6EjlVLGe3VMZEHSAKyW4bG5qQ=s64","userId":"14768070087940992282"}},"outputId":"f5262809-fbff-4b30-cd1c-a691067e98f3"},"source":["# PART 2: TRAIN MODEL\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import math\n","print(torch.__version__)\n","plt.ion()   # interactive mode"],"execution_count":19,"outputs":[{"output_type":"stream","text":["1.9.0+cu102\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K4xlXdEGW1WT","executionInfo":{"status":"ok","timestamp":1629644109569,"user_tz":-180,"elapsed":3,"user":{"displayName":"itamar ginsberg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgNNu4rKWDIJbuQ6EjlVLGe3VMZEHSAKyW4bG5qQ=s64","userId":"14768070087940992282"}}},"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomRotation(5),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize([224,224]),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"O0Iztg0OXaLD","executionInfo":{"status":"error","timestamp":1629644354135,"user_tz":-180,"elapsed":440,"user":{"displayName":"itamar ginsberg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgNNu4rKWDIJbuQ6EjlVLGe3VMZEHSAKyW4bG5qQ=s64","userId":"14768070087940992282"}},"outputId":"74c27d7f-2144-47fe-9f75-b4f5ca080888"},"source":["data_dir = 'data'\n","CHECK_POINT_PATH = 'checkpoint.tar'\n","SUBMISSION_FILE = 'submission.csv'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(class_names) # => ['cats', 'dogs']\n","print(f'Train image size: {dataset_sizes[\"train\"]}')\n","print(f'Validation image size: {dataset_sizes[\"val\"]}')"],"execution_count":30,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-8241836c3340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCHECK_POINT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint.tar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSUBMISSION_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-8241836c3340>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCHECK_POINT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint.tar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSUBMISSION_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"]}]},{"cell_type":"markdown","metadata":{"id":"bDCDKNt1WfiC"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"rnJCOqpeFL12"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"MHnZN64p8fAk","executionInfo":{"status":"error","timestamp":1628615570428,"user_tz":-180,"elapsed":291,"user":{"displayName":"itamar ginsberg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgNNu4rKWDIJbuQ6EjlVLGe3VMZEHSAKyW4bG5qQ=s64","userId":"14768070087940992282"}},"outputId":"e4dc9279-cd57-45cb-8688-298834df4cd8"},"source":["# ## Transfer Learning - thumb classification\n","# ## https://github.com/davidRetana/thumbs_jetbot/blob/master/train_model_thumbs.ipynb\n","\n","\n","# import torch\n","# import torch.optim as optim\n","# import torch.nn.functional as F\n","# import torchvision\n","# import torchvision.datasets as datasets\n","# import torchvision.models as models\n","# import torchvision.transforms as transforms\n","# # Create dataset instance\n","# # Now we use the ImageFolder dataset class available with the torchvision.datasets package. We attach transforms from the torchvision.transforms package to prepare the data for training.\n","\n","\n","# dataset = datasets.ImageFolder(\n","#     'dataset',\n","#     transforms.Compose([\n","#         transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n","#         transforms.Resize((224, 224)),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","#     ])\n","# )\n","# # Split dataset into train and test sets\n","# # Next, we split the dataset into training and test sets. The test set will be used to verify the accuracy of the model we train.\n","\n","# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 50, 50])\n","# # Create data loaders to load data in batches\n","# # We'll create two DataLoader instances, which provide utilities for shuffling data, producing batches of images, and loading the samples in parallel with multiple workers.\n","\n","# train_loader = torch.utils.data.DataLoader(\n","#     train_dataset,\n","#     batch_size=16,\n","#     shuffle=True,\n","#     num_workers=4\n","# )\n","\n","# test_loader = torch.utils.data.DataLoader(\n","#     test_dataset,\n","#     batch_size=16,\n","#     shuffle=True,\n","#     num_workers=4\n","# )\n","# # Define the neural network\n","# # Now, we define the neural network we'll be training. The torchvision package provides a collection of pre-trained models that we can use.\n","\n","# # In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n","\n","# # Important features that were learned in the original training of the pre-trained model are re-usable for the new task. We'll use the alexnet model.\n","\n","# import torchvision.models as models\n","# model = models.alexnet(pretrained=True)\n","\n","# model\n","# # The alexnet model was originally trained for a dataset that had 1000 class labels, but our dataset only has two class labels! We'll replace the final layer with a new, untrained layer that has only two outputs.\n","\n","\n","# model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)\n","# # Finally, we transfer our model for execution on the GPU, if available\n","\n","\n","# device_string = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# device = torch.device(device_string)\n","# model = model.to(device)\n","# # Train the neural network\n","# # Using the code below we will train the neural network for 30 epochs, saving the best performing model after each epoch.\n","\n","# # An epoch is a full run through our data.\n","\n","# NUM_EPOCHS = 30\n","# BEST_MODEL_PATH = 'best_model.pth'\n","# best_accuracy = 0.0\n","\n","# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# for epoch in range(NUM_EPOCHS):\n","    \n","#     for images, labels in iter(train_loader):\n","#         images = images.to(device)\n","#         labels = labels.to(device)\n","#         optimizer.zero_grad()\n","#         outputs = model(images)\n","#         loss = F.cross_entropy(outputs, labels)\n","#         loss.backward()\n","#         optimizer.step()\n","    \n","#     test_error_count = 0.0\n","#     for images, labels in iter(test_loader):\n","#         images = images.to(device)\n","#         labels = labels.to(device)\n","#         outputs = model(images)\n","#         test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n","    \n","#     test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n","#     print('%d: %f' % (epoch, test_accuracy))\n","#     if test_accuracy > best_accuracy:\n","#         torch.save(model.state_dict(), BEST_MODEL_PATH)\n","#         best_accuracy = test_accuracy\n","\n","# # Once that is finished, you should see a file best_model.pth in the Jupyter Lab file browser."],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-2bdad55aaaa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m dataset = datasets.data(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     transforms.Compose([\n","\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.datasets' has no attribute 'data'"]}]}]}